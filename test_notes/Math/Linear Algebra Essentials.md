# Linear Algebra Essentials

## Vectors and Matrices

### Vector Basics
Vectors represent quantities with magnitude and direction. In n-dimensional space, they're ordered lists of numbers.

### Matrix Operations
Matrices are 2D arrays enabling transformations. Key operations include:
- **Addition/Subtraction**: Element-wise operations
- **Multiplication**: Dot products and matrix products
- **Transpose**: Flip rows and columns

### Determinants and Inverses
The determinant measures how a matrix scales space. Invertible matrices have non-zero determinants and unique solutions to linear systems.

## Vector Spaces

### Linear Independence
Vectors are linearly independent if no vector can be expressed as a combination of others. This concept underpins basis vectors.

### Span and Basis
The span of vectors is all possible linear combinations. A basis is a minimal set of linearly independent vectors spanning a space.

### Dimension
The dimension of a vector space equals the number of vectors in any basis for that space.

## Applications in ML

### Feature Representation
Data points become vectors in feature space, enabling mathematical operations and distance metrics.

### Neural Network Computations
Matrix multiplication implements layer transformations. Weights are matrices, activations are vectors, and backpropagation uses gradients.

### Dimensionality Reduction
Techniques like PCA use eigendecomposition to find principal components, reducing data while preserving variance.
