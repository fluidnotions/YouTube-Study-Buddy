{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment Generation Laboratory\n",
    "\n",
    "This notebook explores how YT Study Buddy generates learning assessments that test understanding beyond just note recall.\n",
    "\n",
    "## Learning Goals:\n",
    "- Understand different types of learning assessments\n",
    "- Experiment with prompt engineering for question generation\n",
    "- Create \"one-up\" challenges that promote innovation\n",
    "- Test answer evaluation techniques using semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "# !pip install anthropic sentence-transformers matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Assessment Types\n",
    "\n",
    "YT Study Buddy generates 4 types of questions based on Bloom's Taxonomy and active learning principles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessment types and their cognitive levels\n",
    "assessment_types = {\n",
    "    'Gap Analysis': {\n",
    "        'cognitive_level': 'Understanding/Analysis',\n",
    "        'purpose': 'Test what was missed in summarization',\n",
    "        'example': 'What implementation details were mentioned but not in the notes?',\n",
    "        'bloom_level': 2\n",
    "    },\n",
    "    'Application': {\n",
    "        'cognitive_level': 'Application',\n",
    "        'purpose': 'Test practical use of concepts',\n",
    "        'example': 'How would you apply this technique to solve problem X?',\n",
    "        'bloom_level': 3\n",
    "    },\n",
    "    'One-Up Challenge': {\n",
    "        'cognitive_level': 'Synthesis/Evaluation',\n",
    "        'purpose': 'Promote innovation and improvement',\n",
    "        'example': 'How could you enhance this approach with modern techniques?',\n",
    "        'bloom_level': 5\n",
    "    },\n",
    "    'Synthesis': {\n",
    "        'cognitive_level': 'Synthesis',\n",
    "        'purpose': 'Connect to broader knowledge',\n",
    "        'example': 'How does this relate to other concepts in your knowledge base?',\n",
    "        'bloom_level': 5\n",
    "    }\n",
    "}\n",
    "\n",
    "# Visualize assessment framework\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bloom's taxonomy levels\n",
    "types = list(assessment_types.keys())\n",
    "bloom_levels = [assessment_types[t]['bloom_level'] for t in types]\n",
    "colors = ['lightblue', 'lightgreen', 'orange', 'lightcoral']\n",
    "\n",
    "ax1.barh(types, bloom_levels, color=colors)\n",
    "ax1.set_xlabel('Bloom\\'s Taxonomy Level')\n",
    "ax1.set_title('Assessment Types by Cognitive Complexity')\n",
    "ax1.set_xlim(0, 6)\n",
    "\n",
    "# Purpose distribution\n",
    "purposes = [assessment_types[t]['purpose'] for t in types]\n",
    "ax2.pie([1]*len(types), labels=types, colors=colors, autopct='%1.0f%%')\n",
    "ax2.set_title('Assessment Type Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Assessment Framework Overview:\")\n",
    "for type_name, info in assessment_types.items():\n",
    "    print(f\"\\n{type_name}:\")\n",
    "    print(f\"  Purpose: {info['purpose']}\")\n",
    "    print(f\"  Cognitive Level: {info['cognitive_level']}\")\n",
    "    print(f\"  Example: {info['example']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt Engineering for Question Generation\n",
    "\n",
    "Let's experiment with different prompt structures for generating high-quality questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample video content for testing\n",
    "sample_transcript = \"\"\"\n",
    "In this video, we'll explore transformer neural networks, specifically the attention mechanism \n",
    "that revolutionized natural language processing. The key innovation is self-attention, which \n",
    "allows the model to focus on different parts of the input sequence when processing each element.\n",
    "\n",
    "The transformer uses multi-head attention with 8 attention heads in the original paper. Each head \n",
    "learns different relationships between words. The attention function can be written as \n",
    "Attention(Q,K,V) = softmax(QK^T/√dk)V, where Q is queries, K is keys, and V is values.\n",
    "\n",
    "We also discuss positional encoding, which is crucial because transformers don't have inherent \n",
    "sequence order awareness like RNNs. The encoding uses sine and cosine functions at different \n",
    "frequencies. Layer normalization and residual connections help with training stability.\n",
    "\n",
    "Performance-wise, transformers can be parallelized during training, unlike RNNs which are \n",
    "sequential. However, they have O(n²) memory complexity for sequence length n, which can be \n",
    "problematic for very long sequences. Recent innovations like sparse attention and linear \n",
    "attention try to address this limitation.\n",
    "\"\"\"\n",
    "\n",
    "sample_notes = \"\"\"\n",
    "# Transformer Neural Networks\n",
    "\n",
    "## Core Concepts\n",
    "- Self-attention mechanism allows focusing on different input parts\n",
    "- Multi-head attention with multiple attention heads learning different relationships\n",
    "- Positional encoding provides sequence order information\n",
    "\n",
    "## Key Components\n",
    "- Attention function: Attention(Q,K,V) = softmax(QK^T/√dk)V\n",
    "- Layer normalization and residual connections for training stability\n",
    "- Parallelizable training unlike sequential RNNs\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample Content Loaded:\")\n",
    "print(f\"Transcript length: {len(sample_transcript)} characters\")\n",
    "print(f\"Notes length: {len(sample_notes)} characters\")\n",
    "print(f\"Compression ratio: {len(sample_notes)/len(sample_transcript):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different prompt templates for question generation\n",
    "prompt_templates = {\n",
    "    'basic': \"\"\"\n",
    "Create assessment questions based on this video content:\n",
    "\n",
    "TRANSCRIPT: {transcript}\n",
    "NOTES: {notes}\n",
    "\n",
    "Generate 5 questions testing understanding.\n",
    "\"\"\",\n",
    "    \n",
    "    'structured': \"\"\"\n",
    "Create a learning assessment with specific question types:\n",
    "\n",
    "VIDEO CONTENT: {transcript}\n",
    "SUMMARY NOTES: {notes}\n",
    "\n",
    "Generate exactly 5 questions:\n",
    "1. Gap Analysis: What important details were in the video but NOT in the notes?\n",
    "2. Application: How would you apply these concepts practically?\n",
    "3. One-Up Challenge: How could you improve or extend the discussed approach?\n",
    "4. Synthesis: How do these concepts connect to other areas?\n",
    "5. Critical Thinking: What are potential limitations or challenges?\n",
    "\"\"\",\n",
    "    \n",
    "    'bloom_guided': \"\"\"\n",
    "Using Bloom's Taxonomy, create questions at different cognitive levels:\n",
    "\n",
    "CONTENT: {transcript}\n",
    "NOTES: {notes}\n",
    "\n",
    "Level 2 (Understanding): What key concepts were explained?\n",
    "Level 3 (Application): How would you implement this in practice?\n",
    "Level 4 (Analysis): What are the trade-offs and limitations?\n",
    "Level 5 (Synthesis): How does this connect to other technologies?\n",
    "Level 6 (Evaluation): How would you improve this approach?\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "print(\"Prompt Templates Created:\")\n",
    "for name, template in prompt_templates.items():\n",
    "    print(f\"\\n{name.upper()} Template:\")\n",
    "    print(f\"Length: {len(template)} characters\")\n",
    "    print(f\"Focus: {template.split(':', 1)[0] if ':' in template else 'General'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mock Question Generation\n",
    "\n",
    "Since we can't call the Claude API directly in this notebook, let's create mock questions for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock generated questions for testing evaluation\n",
    "mock_questions = {\n",
    "    \"gap_analysis\": [\n",
    "        {\n",
    "            \"question\": \"What specific technical details about transformer complexity were mentioned in the video but not captured in the notes?\",\n",
    "            \"model_answer\": \"The video mentioned O(n²) memory complexity for sequence length n, and recent innovations like sparse attention and linear attention to address this limitation. It also specified that the original transformer used 8 attention heads.\",\n",
    "            \"concepts\": [\"computational complexity\", \"memory usage\", \"recent innovations\"]\n",
    "        }\n",
    "    ],\n",
    "    \"application\": [\n",
    "        {\n",
    "            \"question\": \"How would you implement positional encoding in a transformer model for a new language that reads right-to-left?\",\n",
    "            \"model_answer\": \"You would need to reverse the positional encoding order or modify the sine/cosine function parameters to account for right-to-left reading direction. The key is maintaining consistent positional relationships.\",\n",
    "            \"concepts\": [\"positional encoding\", \"implementation\", \"language adaptation\"]\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Given a sequence length limit due to memory constraints, how would you adapt a transformer for processing very long documents?\",\n",
    "            \"model_answer\": \"You could implement hierarchical attention, sliding window attention, or chunking strategies. Another approach is using sparse attention patterns or recent linear attention mechanisms to reduce the O(n²) complexity.\",\n",
    "            \"concepts\": [\"memory optimization\", \"sparse attention\", \"scalability\"]\n",
    "        }\n",
    "    ],\n",
    "    \"one_up\": [\n",
    "        {\n",
    "            \"question\": \"How would you enhance the transformer architecture to incorporate visual information alongside text for multimodal understanding?\",\n",
    "            \"model_answer\": \"You could add vision encoders (like CNNs or Vision Transformers) and create cross-modal attention mechanisms. The visual features would need to be projected into the same embedding space as text tokens.\",\n",
    "            \"concepts\": [\"multimodal learning\", \"cross-modal attention\", \"vision integration\"]\n",
    "        }\n",
    "    ],\n",
    "    \"synthesis\": [\n",
    "        {\n",
    "            \"question\": \"How do transformer attention mechanisms relate to human cognitive attention processes?\",\n",
    "            \"model_answer\": \"Both involve selective focus on relevant information while ignoring irrelevant details. However, transformer attention is learned through training data patterns, while human attention involves conscious control and emotional factors.\",\n",
    "            \"concepts\": [\"cognitive science\", \"attention theory\", \"human-AI comparison\"]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display generated questions\n",
    "print(\"Mock Assessment Questions:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "question_num = 1\n",
    "for category, questions in mock_questions.items():\n",
    "    category_title = category.replace('_', ' ').title()\n",
    "    print(f\"\\n{category_title}:\")\n",
    "    \n",
    "    for q in questions:\n",
    "        print(f\"\\n{question_num}. {q['question']}\")\n",
    "        print(f\"   Concepts: {', '.join(q['concepts'])}\")\n",
    "        question_num += 1\n",
    "\n",
    "total_questions = sum(len(questions) for questions in mock_questions.values())\n",
    "print(f\"\\nTotal Questions Generated: {total_questions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Answer Evaluation System\n",
    "\n",
    "Let's build a system to evaluate user answers using semantic similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence transformer for answer evaluation\n",
    "print(\"Loading sentence transformer for answer evaluation...\")\n",
    "evaluator_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "def evaluate_answer(user_answer, model_answer, threshold=0.7):\n",
    "    \"\"\"Evaluate user answer against model answer using semantic similarity.\"\"\"\n",
    "    \n",
    "    # Encode both answers\n",
    "    user_embedding = evaluator_model.encode([user_answer])\n",
    "    model_embedding = evaluator_model.encode([model_answer])\n",
    "    \n",
    "    # Calculate similarity\n",
    "    similarity = cosine_similarity(user_embedding, model_embedding)[0][0]\n",
    "    \n",
    "    # Generate feedback based on similarity\n",
    "    if similarity >= 0.8:\n",
    "        feedback = \"Excellent! Your answer closely matches the expected response.\"\n",
    "        grade = \"A\"\n",
    "    elif similarity >= 0.7:\n",
    "        feedback = \"Good understanding! Your answer covers the main points.\"\n",
    "        grade = \"B\"\n",
    "    elif similarity >= 0.5:\n",
    "        feedback = \"Partial understanding. You've got some key concepts but missing details.\"\n",
    "        grade = \"C\"\n",
    "    elif similarity >= 0.3:\n",
    "        feedback = \"Some relevant points, but significant gaps in understanding.\"\n",
    "        grade = \"D\"\n",
    "    else:\n",
    "        feedback = \"Answer doesn't align well with expected response. Consider reviewing the content.\"\n",
    "        grade = \"F\"\n",
    "    \n",
    "    return {\n",
    "        'similarity': similarity,\n",
    "        'grade': grade,\n",
    "        'feedback': feedback\n",
    "    }\n",
    "\n",
    "print(\"Answer evaluation system ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test answer evaluation with sample responses\n",
    "test_question = mock_questions[\"gap_analysis\"][0]\n",
    "model_answer = test_question[\"model_answer\"]\n",
    "\n",
    "# Sample user answers of varying quality\n",
    "sample_answers = {\n",
    "    \"Excellent\": \"The video discussed O(n²) memory complexity and mentioned sparse attention and linear attention as recent solutions. It also specified 8 attention heads in the original transformer.\",\n",
    "    \n",
    "    \"Good\": \"It mentioned memory complexity issues and some recent techniques like sparse attention to improve efficiency.\",\n",
    "    \n",
    "    \"Average\": \"There were some technical details about performance and memory that weren't in the notes.\",\n",
    "    \n",
    "    \"Poor\": \"The video talked about transformers and attention mechanisms.\",\n",
    "    \n",
    "    \"Irrelevant\": \"I think neural networks are really interesting and have many applications in machine learning.\"\n",
    "}\n",
    "\n",
    "print(f\"Question: {test_question['question']}\")\n",
    "print(f\"\\nModel Answer: {model_answer}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANSWER EVALUATION RESULTS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "for quality, answer in sample_answers.items():\n",
    "    evaluation = evaluate_answer(answer, model_answer)\n",
    "    results.append((quality, evaluation['similarity'], evaluation['grade']))\n",
    "    \n",
    "    print(f\"\\n{quality} Answer:\")\n",
    "    print(f\"User: {answer}\")\n",
    "    print(f\"Similarity: {evaluation['similarity']:.3f} | Grade: {evaluation['grade']} | {evaluation['feedback']}\")\n",
    "\n",
    "# Visualize evaluation results\n",
    "qualities, similarities, grades = zip(*results)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "colors = ['green', 'lightgreen', 'yellow', 'orange', 'red']\n",
    "plt.bar(qualities, similarities, color=colors)\n",
    "plt.axhline(y=0.7, color='red', linestyle='--', alpha=0.7, label='Passing Threshold')\n",
    "plt.xlabel('Answer Quality')\n",
    "plt.ylabel('Semantic Similarity')\n",
    "plt.title('Answer Evaluation by Semantic Similarity')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "grade_counts = {grade: grades.count(grade) for grade in set(grades)}\n",
    "plt.pie(grade_counts.values(), labels=grade_counts.keys(), autopct='%1.0f%%')\n",
    "plt.title('Grade Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. One-Up Challenge Generator\n",
    "\n",
    "The \"one-up\" challenge is the most innovative part of the assessment system. Let's explore how to create these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-up challenge templates\n",
    "one_up_templates = {\n",
    "    'efficiency': \"The video shows {basic_approach}. How could you optimize this for {constraint}?\",\n",
    "    'scale': \"This works for {current_scale}. How would you modify it to handle {larger_scale}?\",\n",
    "    'innovation': \"The video uses {standard_technique}. What if you combined it with {emerging_technique}?\",\n",
    "    'integration': \"How would you integrate this approach with {other_domain} to create a hybrid solution?\",\n",
    "    'modernization': \"This technique was developed in {time_period}. How would you update it using {modern_tools}?\"\n",
    "}\n",
    "\n",
    "# Context extraction for template filling\n",
    "def extract_challenge_context(transcript, video_topic):\n",
    "    \"\"\"Extract context information for generating one-up challenges.\"\"\"\n",
    "    \n",
    "    # This would typically use NLP to extract these, but we'll simulate\n",
    "    context = {\n",
    "        'basic_approach': 'standard transformer architecture',\n",
    "        'constraint': 'mobile devices with limited memory',\n",
    "        'current_scale': 'sequences up to 512 tokens',\n",
    "        'larger_scale': 'full-length documents (10k+ tokens)',\n",
    "        'standard_technique': 'multi-head attention',\n",
    "        'emerging_technique': 'sparse attention patterns',\n",
    "        'other_domain': 'computer vision',\n",
    "        'time_period': '2017',\n",
    "        'modern_tools': '2024 optimization techniques',\n",
    "        'video_topic': video_topic\n",
    "    }\n",
    "    \n",
    "    return context\n",
    "\n",
    "# Generate one-up challenges\n",
    "def generate_one_up_challenges(transcript, video_topic=\"Transformer Neural Networks\"):\n",
    "    \"\"\"Generate innovative one-up challenges.\"\"\"\n",
    "    \n",
    "    context = extract_challenge_context(transcript, video_topic)\n",
    "    challenges = []\n",
    "    \n",
    "    for challenge_type, template in one_up_templates.items():\n",
    "        try:\n",
    "            challenge = template.format(**context)\n",
    "            challenges.append({\n",
    "                'type': challenge_type,\n",
    "                'challenge': challenge,\n",
    "                'difficulty': 'Advanced',\n",
    "                'innovation_level': 'High'\n",
    "            })\n",
    "        except KeyError as e:\n",
    "            print(f\"Missing context for {challenge_type}: {e}\")\n",
    "    \n",
    "    return challenges\n",
    "\n",
    "# Generate challenges for our sample content\n",
    "one_up_challenges = generate_one_up_challenges(sample_transcript)\n",
    "\n",
    "print(\"🚀 ONE-UP CHALLENGES:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, challenge in enumerate(one_up_challenges, 1):\n",
    "    print(f\"\\n{i}. {challenge['type'].title()} Challenge:\")\n",
    "    print(f\"   {challenge['challenge']}\")\n",
    "    print(f\"   Difficulty: {challenge['difficulty']} | Innovation: {challenge['innovation_level']}\")\n",
    "\n",
    "print(f\"\\n💡 These challenges push beyond comprehension to creative problem-solving!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Assessment Quality Metrics\n",
    "\n",
    "Let's define metrics to evaluate the quality of generated assessments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_question_quality(question, model_answer, concepts):\n",
    "    \"\"\"Evaluate the quality of a generated question.\"\"\"\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Complexity (based on word count and technical terms)\n",
    "    question_words = len(question.split())\n",
    "    answer_words = len(model_answer.split())\n",
    "    \n",
    "    metrics['complexity_score'] = min(10, (question_words + answer_words) / 10)\n",
    "    \n",
    "    # Concept coverage\n",
    "    metrics['concept_count'] = len(concepts)\n",
    "    metrics['concept_score'] = min(10, len(concepts) * 2)\n",
    "    \n",
    "    # Question type indicators\n",
    "    innovation_words = ['improve', 'enhance', 'optimize', 'extend', 'innovate', 'combine']\n",
    "    application_words = ['apply', 'implement', 'use', 'practice', 'real-world']\n",
    "    analysis_words = ['analyze', 'compare', 'evaluate', 'assess', 'limitations']\n",
    "    \n",
    "    question_lower = question.lower()\n",
    "    \n",
    "    metrics['innovation_indicators'] = sum(1 for word in innovation_words if word in question_lower)\n",
    "    metrics['application_indicators'] = sum(1 for word in application_words if word in question_lower)\n",
    "    metrics['analysis_indicators'] = sum(1 for word in analysis_words if word in question_lower)\n",
    "    \n",
    "    # Overall quality score\n",
    "    quality_components = [\n",
    "        metrics['complexity_score'] * 0.3,\n",
    "        metrics['concept_score'] * 0.4,\n",
    "        (metrics['innovation_indicators'] + metrics['application_indicators'] + metrics['analysis_indicators']) * 0.3\n",
    "    ]\n",
    "    \n",
    "    metrics['overall_quality'] = sum(quality_components)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Evaluate all mock questions\n",
    "question_evaluations = []\n",
    "\n",
    "for category, questions in mock_questions.items():\n",
    "    for q in questions:\n",
    "        metrics = assess_question_quality(q['question'], q['model_answer'], q['concepts'])\n",
    "        metrics['category'] = category\n",
    "        metrics['question'] = q['question'][:50] + \"...\" if len(q['question']) > 50 else q['question']\n",
    "        question_evaluations.append(metrics)\n",
    "\n",
    "# Visualize question quality\n",
    "categories = [q['category'] for q in question_evaluations]\n",
    "quality_scores = [q['overall_quality'] for q in question_evaluations]\n",
    "concept_counts = [q['concept_count'] for q in question_evaluations]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Quality by category\n",
    "axes[0,0].bar(range(len(categories)), quality_scores, color=['blue', 'green', 'orange', 'red', 'purple'][:len(categories)])\n",
    "axes[0,0].set_xlabel('Question Index')\n",
    "axes[0,0].set_ylabel('Quality Score')\n",
    "axes[0,0].set_title('Question Quality Scores')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Concept coverage\n",
    "axes[0,1].scatter(concept_counts, quality_scores, c=['blue', 'green', 'orange', 'red', 'purple'][:len(categories)], s=100)\n",
    "axes[0,1].set_xlabel('Number of Concepts')\n",
    "axes[0,1].set_ylabel('Quality Score')\n",
    "axes[0,1].set_title('Quality vs Concept Coverage')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Category distribution\n",
    "category_counts = {cat: categories.count(cat) for cat in set(categories)}\n",
    "axes[1,0].pie(category_counts.values(), labels=category_counts.keys(), autopct='%1.0f%%')\n",
    "axes[1,0].set_title('Question Category Distribution')\n",
    "\n",
    "# Innovation indicators\n",
    "innovation_scores = [q['innovation_indicators'] for q in question_evaluations]\n",
    "axes[1,1].bar(range(len(categories)), innovation_scores, color=['lightblue', 'lightgreen', 'yellow', 'pink', 'lightgray'][:len(categories)])\n",
    "axes[1,1].set_xlabel('Question Index')\n",
    "axes[1,1].set_ylabel('Innovation Indicators')\n",
    "axes[1,1].set_title('Innovation Focus by Question')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "avg_quality = np.mean(quality_scores)\n",
    "avg_concepts = np.mean(concept_counts)\n",
    "total_innovation = sum(innovation_scores)\n",
    "\n",
    "print(f\"\\n📊 ASSESSMENT QUALITY METRICS:\")\n",
    "print(f\"Average Quality Score: {avg_quality:.2f}/10\")\n",
    "print(f\"Average Concepts per Question: {avg_concepts:.1f}\")\n",
    "print(f\"Total Innovation Indicators: {total_innovation}\")\n",
    "print(f\"Questions Generated: {len(question_evaluations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Assessment Builder\n",
    "\n",
    "Create your own assessment questions and test the evaluation system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive assessment creation\n",
    "def create_custom_assessment():\n",
    "    \"\"\"Interactive tool for creating custom assessments.\"\"\"\n",
    "    \n",
    "    print(\"🎯 CUSTOM ASSESSMENT BUILDER\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get user input (in a real notebook, you'd use widgets)\n",
    "    sample_questions = {\n",
    "        \"gap_analysis\": \"What important performance details were mentioned but not summarized?\",\n",
    "        \"application\": \"How would you implement this for a real-time system?\",\n",
    "        \"one_up\": \"How could you enhance this with quantum computing concepts?\",\n",
    "        \"synthesis\": \"How does this relate to graph neural networks?\"\n",
    "    }\n",
    "    \n",
    "    sample_answers = {\n",
    "        \"gap_analysis\": \"The O(n²) complexity and sparse attention solutions were key details not captured in the summary.\",\n",
    "        \"application\": \"You'd need to implement attention caching and optimize matrix operations for low-latency inference.\",\n",
    "        \"one_up\": \"Quantum attention mechanisms could potentially achieve exponential speedups for certain patterns.\",\n",
    "        \"synthesis\": \"Both use attention mechanisms but GNNs focus on node relationships while transformers handle sequences.\"\n",
    "    }\n",
    "    \n",
    "    print(\"Sample Custom Questions Created:\")\n",
    "    \n",
    "    for qtype, question in sample_questions.items():\n",
    "        print(f\"\\n{qtype.replace('_', ' ').title()}:\")\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"A: {sample_answers[qtype]}\")\n",
    "        \n",
    "        # Evaluate question quality\n",
    "        concepts = ['attention', 'complexity', 'optimization']  # Simplified\n",
    "        quality = assess_question_quality(question, sample_answers[qtype], concepts)\n",
    "        print(f\"Quality Score: {quality['overall_quality']:.1f}/10\")\n",
    "\n",
    "create_custom_assessment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Evaluation Techniques\n",
    "\n",
    "Explore more sophisticated answer evaluation methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced evaluation combining multiple signals\n",
    "def advanced_answer_evaluation(user_answer, model_answer, question, concepts):\n",
    "    \"\"\"Multi-faceted answer evaluation.\"\"\"\n",
    "    \n",
    "    # Semantic similarity (primary)\n",
    "    semantic_score = cosine_similarity(\n",
    "        evaluator_model.encode([user_answer]),\n",
    "        evaluator_model.encode([model_answer])\n",
    "    )[0][0]\n",
    "    \n",
    "    # Concept coverage\n",
    "    user_lower = user_answer.lower()\n",
    "    concepts_mentioned = sum(1 for concept in concepts if concept.lower() in user_lower)\n",
    "    concept_coverage = concepts_mentioned / len(concepts) if concepts else 0\n",
    "    \n",
    "    # Answer length appropriateness\n",
    "    user_words = len(user_answer.split())\n",
    "    model_words = len(model_answer.split())\n",
    "    length_ratio = min(user_words / model_words, model_words / user_words) if model_words > 0 else 0\n",
    "    \n",
    "    # Technical depth (presence of technical terms)\n",
    "    technical_terms = ['algorithm', 'complexity', 'optimization', 'implementation', 'architecture', \n",
    "                      'neural', 'attention', 'embedding', 'matrix', 'vector', 'model', 'training']\n",
    "    tech_terms_used = sum(1 for term in technical_terms if term in user_lower)\n",
    "    technical_depth = min(tech_terms_used / 5, 1.0)  # Normalize to 0-1\n",
    "    \n",
    "    # Weighted composite score\n",
    "    weights = {\n",
    "        'semantic': 0.4,\n",
    "        'concepts': 0.3,\n",
    "        'length': 0.1,\n",
    "        'technical': 0.2\n",
    "    }\n",
    "    \n",
    "    composite_score = (\n",
    "        semantic_score * weights['semantic'] +\n",
    "        concept_coverage * weights['concepts'] +\n",
    "        length_ratio * weights['length'] +\n",
    "        technical_depth * weights['technical']\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'semantic_similarity': semantic_score,\n",
    "        'concept_coverage': concept_coverage,\n",
    "        'length_appropriateness': length_ratio,\n",
    "        'technical_depth': technical_depth,\n",
    "        'composite_score': composite_score,\n",
    "        'detailed_feedback': generate_detailed_feedback(semantic_score, concept_coverage, technical_depth)\n",
    "    }\n",
    "\n",
    "def generate_detailed_feedback(semantic_score, concept_coverage, technical_depth):\n",
    "    \"\"\"Generate detailed feedback based on multiple evaluation criteria.\"\"\"\n",
    "    \n",
    "    feedback_parts = []\n",
    "    \n",
    "    if semantic_score >= 0.8:\n",
    "        feedback_parts.append(\"✅ Excellent semantic alignment with expected answer\")\n",
    "    elif semantic_score >= 0.6:\n",
    "        feedback_parts.append(\"👍 Good understanding of core concepts\")\n",
    "    else:\n",
    "        feedback_parts.append(\"⚠️ Consider reviewing key concepts - answer doesn't align well\")\n",
    "    \n",
    "    if concept_coverage >= 0.8:\n",
    "        feedback_parts.append(\"🎯 Comprehensive concept coverage\")\n",
    "    elif concept_coverage >= 0.5:\n",
    "        feedback_parts.append(\"📝 Good concept coverage, could include more key terms\")\n",
    "    else:\n",
    "        feedback_parts.append(\"📚 Missing important concepts - review the material\")\n",
    "    \n",
    "    if technical_depth >= 0.6:\n",
    "        feedback_parts.append(\"🔬 Strong technical depth\")\n",
    "    elif technical_depth >= 0.3:\n",
    "        feedback_parts.append(\"⚙️ Some technical detail, could be more specific\")\n",
    "    else:\n",
    "        feedback_parts.append(\"🔧 Add more technical specificity to strengthen answer\")\n",
    "    \n",
    "    return \" | \".join(feedback_parts)\n",
    "\n",
    "# Test advanced evaluation\n",
    "test_question = \"What performance optimizations were discussed?\"\n",
    "test_concepts = ['complexity', 'optimization', 'memory']\n",
    "model_ans = \"O(n²) memory complexity and sparse attention optimizations\"\n",
    "\n",
    "test_answers = [\n",
    "    \"The video mentioned O(n²) complexity issues and sparse attention as optimization techniques\",\n",
    "    \"There were some performance improvements discussed\",\n",
    "    \"Transformers have quadratic memory complexity which can be addressed through sparse attention patterns and linear attention mechanisms for better scalability\"\n",
    "]\n",
    "\n",
    "print(\"🔬 ADVANCED EVALUATION RESULTS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, answer in enumerate(test_answers, 1):\n",
    "    evaluation = advanced_answer_evaluation(answer, model_ans, test_question, test_concepts)\n",
    "    \n",
    "    print(f\"\\nAnswer {i}: {answer}\")\n",
    "    print(f\"Semantic: {evaluation['semantic_similarity']:.3f} | \"\n",
    "          f\"Concepts: {evaluation['concept_coverage']:.3f} | \"\n",
    "          f\"Technical: {evaluation['technical_depth']:.3f}\")\n",
    "    print(f\"Composite Score: {evaluation['composite_score']:.3f}\")\n",
    "    print(f\"Feedback: {evaluation['detailed_feedback']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Key Takeaways\n",
    "\n",
    "1. **Assessment Types**: Gap analysis, application, one-up challenges, and synthesis questions test different cognitive levels\n",
    "2. **Prompt Engineering**: Structured prompts with clear categories generate better questions\n",
    "3. **Semantic Evaluation**: Sentence transformers provide robust answer evaluation beyond keyword matching\n",
    "4. **Innovation Focus**: \"One-up\" challenges push learners beyond comprehension to creative problem-solving\n",
    "5. **Quality Metrics**: Multi-faceted evaluation considers semantics, concept coverage, and technical depth\n",
    "\n",
    "## 🚀 Next Steps\n",
    "\n",
    "- Experiment with different question generation prompts\n",
    "- Fine-tune evaluation thresholds based on your domain\n",
    "- Create domain-specific concept vocabularies\n",
    "- Implement progressive difficulty scaling\n",
    "- Add peer review and self-assessment features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}